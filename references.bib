@inproceedings{goyal_Recurrent_2020a,
  title = {Recurrent {{Independent Mechanisms}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Goyal, Anirudh and Lamb, Alex and Hoffmann, Jordan and Sodhani, Shagun and Levine, Sergey and Bengio, Yoshua and Sch{\"o}lkopf, Bernhard},
  year = {2020},
  month = sep,
  langid = {english},
  url = {https://openreview.net/forum?id=mLcmdlEUxy-}
}

@article{zimmermann2021nested,
  title={Nested Variational Inference},
  author={Zimmermann, Heiko and Wu, Hao and Esmaeili, Babak and van de Meent, Jan-Willem},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021},
  url = {https://proceedings.neurips.cc/paper/2021/hash/ab49b208848abe14418090d95df0d590-Abstract.html}
}

@article{girin2020dynamical,
  title={Dynamical variational autoencoders: A comprehensive review},
  author={Girin, Laurent and Leglaive, Simon and Bie, Xiaoyu and Diard, Julien and Hueber, Thomas and Alameda-Pineda, Xavier},
  journal={arXiv preprint arXiv:2008.12595},
  year={2020},
  url = {https://arxiv.org/abs/2008.12595}
}

@inproceedings{frankle2018lottery,
  title={The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks},
  author={Frankle, Jonathan and Carbin, Michael},
  booktitle={International Conference on Learning Representations},
  year={2018},
  url = {https://openreview.net/forum?id=rJl-b3RcF7}
}

@article{malach2020implications,
  title={The implications of local correlation on learning some deep functions},
  author={Malach, Eran and Shalev-Shwartz, Shai},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={1322--1332},
  year={2020},
  url = {https://proceedings.neurips.cc/paper/2020/hash/0e4ceef65add6cf21c0f3f9da53b71c0-Abstract.html}
}

@article{bronstein2021geometric,
  title={Geometric deep learning: Grids, groups, graphs, geodesics, and gauges},
  author={Bronstein, Michael M and Bruna, Joan and Cohen, Taco and Veli{\v{c}}kovi{\'c}, Petar},
  journal={arXiv preprint arXiv:2104.13478},
  year={2021},
  url = {https://arxiv.org/abs/2104.13478}
}

@article{keller2021topographic,
  title={Topographic {VAE}s learn equivariant capsules},
  author={Keller, Thomas and Welling, Max},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021},
  url = {https://proceedings.neurips.cc/paper/2021/hash/f03704cb51f02f80b09bffba15751691-Abstract.html}
}

@article{decelle2021equilibrium,
  title={Equilibrium and non-equilibrium regimes in the learning of restricted Boltzmann machines},
  author={Decelle, Aur{\'e}lien and Furtlehner, Cyril and Seoane, Beatriz},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021},
  url = {https://proceedings.neurips.cc/paper/2021/hash/2aedcba61ca55ceb62d785c6b7f10a83-Abstract.html}
}

@article{richthofer2017pfax,
  title={PFAx: Predictable Feature Analysis to Perform Control},
  author={Richthofer, Stefan and Wiskott, Laurenz},
  journal={arXiv preprint arXiv:1712.00634},
  year={2017},
  url = {https://arxiv.org/abs/1712.00634}
}

@article{wiskott2002slow,
  title={Slow feature analysis: Unsupervised learning of invariances},
  author={Wiskott, Laurenz and Sejnowski, Terrence J},
  journal={Neural computation},
  volume={14},
  number={4},
  pages={715--770},
  year={2002},
  publisher={MIT Press},
  url = {https://direct.mit.edu/neco/article-abstract/14/4/715/6583/Slow-Feature-Analysis-Unsupervised-Learning-of?redirectedFrom=fulltext}
}

@article{seth_theories_2022,
	title = {Theories of consciousness},
	rights = {2022 Springer Nature Limited},
	issn = {1471-0048},
	url = {https://www.nature.com/articles/s41583-022-00587-4},
	doi = {10.1038/s41583-022-00587-4},
	abstract = {Recent years have seen a blossoming of theories about the biological and physical basis of consciousness. Good theories guide empirical research, allowing us to interpret data, develop new experimental techniques and expand our capacity to manipulate the phenomenon of interest. Indeed, it is only when couched in terms of a theory that empirical discoveries can ultimately deliver a satisfying understanding of a phenomenon. However, in the case of consciousness, it is unclear how current theories relate to each other, or whether they can be empirically distinguished. To clarify this complicated landscape, we review four prominent theoretical approaches to consciousness: higher-order theories, global workspace theories, re-entry and predictive processing theories and integrated information theory. We describe the key characteristics of each approach by identifying which aspects of consciousness they propose to explain, what their neurobiological commitments are and what empirical data are adduced in their support. We consider how some prominent empirical debates might distinguish among these theories, and we outline three ways in which theories need to be developed to deliver a mature regimen of theory-testing in the neuroscience of consciousness. There are good reasons to think that the iterative development, testing and comparison of theories of consciousness will lead to a deeper understanding of this most profound of mysteries.},
	pages = {1--14},
	journaltitle = {Nature Reviews Neuroscience},
	shortjournal = {Nat Rev Neurosci},
	author = {Seth, Anil K. and Bayne, Tim},
	urldate = {2022-05-11},
	date = {2022-05-03},
	langid = {english},
	note = {Publisher: Nature Publishing Group},
	keywords = {Consciousness, Neuroscience},
	file = {Full Text PDF:/Users/work/Zotero/storage/6SZH73I6/Seth and Bayne - 2022 - Theories of consciousness.pdf:application/pdf;Snapshot:/Users/work/Zotero/storage/8KKHLGLB/s41583-022-00587-4.html:text/html},
}

@report{khan_bayesian_2022,
	title = {The Bayesian Learning Rule},
	url = {http://arxiv.org/abs/2107.04562},
	abstract = {We show that many machine-learning algorithms are specific instances of a single algorithm called the Bayesian learning rule. The rule, derived from Bayesian principles, yields a wide-range of algorithms from fields such as optimization, deep learning, and graphical models. This includes classical algorithms such as ridge regression, Newton's method, and Kalman filter, as well as modern deep-learning algorithms such as stochastic-gradient descent, {RMSprop}, and Dropout. The key idea in deriving such algorithms is to approximate the posterior using candidate distributions estimated by using natural gradients. Different candidate distributions result in different algorithms and further approximations to natural gradients give rise to variants of those algorithms. Our work not only unifies, generalizes, and improves existing algorithms, but also helps us design new ones.},
	number = {{arXiv}:2107.04562},
	institution = {{arXiv}},
	author = {Khan, Mohammad Emtiyaz and Rue, HÃ¥vard},
	urldate = {2022-06-01},
	date = {2022-03-18},
	doi = {10.48550/arXiv.2107.04562},
	eprinttype = {arxiv},
	eprint = {2107.04562 [cs, stat]},
	note = {type: article},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/work/Zotero/storage/BHNNJV77/Khan and Rue - 2022 - The Bayesian Learning Rule.pdf:application/pdf;arXiv.org Snapshot:/Users/work/Zotero/storage/6X2KQ3Y2/2107.html:text/html},
}

@report{zhou_fortuitous_2022,
	title = {Fortuitous Forgetting in Connectionist Networks},
	url = {http://arxiv.org/abs/2202.00155},
	abstract = {Forgetting is often seen as an unwanted characteristic in both human and machine learning. However, we propose that forgetting can in fact be favorable to learning. We introduce "forget-and-relearn" as a powerful paradigm for shaping the learning trajectories of artificial neural networks. In this process, the forgetting step selectively removes undesirable information from the model, and the relearning step reinforces features that are consistently useful under different conditions. The forget-and-relearn framework unifies many existing iterative training algorithms in the image classification and language emergence literature, and allows us to understand the success of these algorithms in terms of the disproportionate forgetting of undesirable information. We leverage this understanding to improve upon existing algorithms by designing more targeted forgetting operations. Insights from our analysis provide a coherent view on the dynamics of iterative training in neural networks and offer a clear path towards performance improvements.},
	number = {{arXiv}:2202.00155},
	institution = {{arXiv}},
	author = {Zhou, Hattie and Vani, Ankit and Larochelle, Hugo and Courville, Aaron},
	urldate = {2022-06-01},
	date = {2022-01-31},
	doi = {10.48550/arXiv.2202.00155},
	eprinttype = {arxiv},
	eprint = {2202.00155 [cs]},
	note = {type: article},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:/Users/work/Zotero/storage/TZI5ZV68/Zhou et al. - 2022 - Fortuitous Forgetting in Connectionist Networks.pdf:application/pdf;arXiv.org Snapshot:/Users/work/Zotero/storage/ZAAW3QWY/2202.html:text/html},
}

@report{lambert_variational_2022,
	title = {Variational inference via Wasserstein gradient flows},
	url = {http://arxiv.org/abs/2205.15902},
	abstract = {Along with Markov chain Monte Carlo ({MCMC}) methods, variational inference ({VI}) has emerged as a central computational approach to large-scale Bayesian inference. Rather than sampling from the true posterior \${\textbackslash}pi\$, {VI} aims at producing a simple but effective approximation \${\textbackslash}hat {\textbackslash}pi\$ to \${\textbackslash}pi\$ for which summary statistics are easy to compute. However, unlike the well-studied {MCMC} methodology, {VI} is still poorly understood and dominated by heuristics. In this work, we propose principled methods for {VI}, in which \${\textbackslash}hat {\textbackslash}pi\$ is taken to be a Gaussian or a mixture of Gaussians, which rest upon the theory of gradient flows on the Bures-Wasserstein space of Gaussian measures. Akin to {MCMC}, it comes with strong theoretical guarantees when \${\textbackslash}pi\$ is log-concave.},
	number = {{arXiv}:2205.15902},
	institution = {{arXiv}},
	author = {Lambert, Marc and Chewi, Sinho and Bach, Francis and Bonnabel, SilvÃ¨re and Rigollet, Philippe},
	urldate = {2022-06-01},
	date = {2022-05-31},
	doi = {10.48550/arXiv.2205.15902},
	eprinttype = {arxiv},
	eprint = {2205.15902 [cs, math, stat]},
	note = {type: article},
	keywords = {Computer Science - Machine Learning, Mathematics - Statistics Theory, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/work/Zotero/storage/UNBSQZ72/Lambert et al. - 2022 - Variational inference via Wasserstein gradient flo.pdf:application/pdf;arXiv.org Snapshot:/Users/work/Zotero/storage/LYC8HITV/2205.html:text/html},
}

@report{mediano_greater_2021,
	title = {Greater than the parts: A review of the information decomposition approach to causal emergence},
	url = {http://arxiv.org/abs/2111.06518},
	shorttitle = {Greater than the parts},
	abstract = {Emergence is a profound subject that straddles many scientific disciplines, including the formation of galaxies and how consciousness arises from the collective activity of neurons. Despite the broad interest that exists on this concept, the study of emergence has suffered from a lack of formalisms that could be used to guide discussions and advance theories. Here we summarise, elaborate on, and extend a recent formal theory of causal emergence based on information decomposition, which is quantifiable and amenable to empirical testing. This theory relates emergence with information about a system's temporal evolution that cannot be obtained from the parts of the system separately. This article provides an accessible but rigorous introduction to the framework, discussing the merits of the approach in various scenarios of interest. We also discuss several interpretation issues and potential misunderstandings, while highlighting the distinctive benefits of this formalism.},
	number = {{arXiv}:2111.06518},
	institution = {{arXiv}},
	author = {Mediano, Pedro A. M. and Rosas, Fernando E. and Luppi, Andrea I. and Jensen, Henrik J. and Seth, Anil K. and Barrett, Adam B. and Carhart-Harris, Robin L. and Bor, Daniel},
	urldate = {2022-06-01},
	date = {2021-11-11},
	doi = {10.48550/arXiv.2111.06518},
	eprinttype = {arxiv},
	eprint = {2111.06518 [nlin, q-bio]},
	note = {type: article},
	keywords = {Nonlinear Sciences - Adaptation and Self-Organizing Systems, Quantitative Biology - Neurons and Cognition},
	file = {arXiv Fulltext PDF:/Users/work/Zotero/storage/ATBXUYSI/Mediano et al. - 2021 - Greater than the parts A review of the informatio.pdf:application/pdf;arXiv.org Snapshot:/Users/work/Zotero/storage/2VUAZBN7/2111.html:text/html},
}