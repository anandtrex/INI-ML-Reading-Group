@inproceedings{goyal_Recurrent_2020a,
  title = {Recurrent {{Independent Mechanisms}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Goyal, Anirudh and Lamb, Alex and Hoffmann, Jordan and Sodhani, Shagun and Levine, Sergey and Bengio, Yoshua and Sch{\"o}lkopf, Bernhard},
  year = {2020},
  month = sep,
  langid = {english},
  url = {https://openreview.net/forum?id=mLcmdlEUxy-}
}

@article{zimmermann2021nested,
  title={Nested Variational Inference},
  author={Zimmermann, Heiko and Wu, Hao and Esmaeili, Babak and van de Meent, Jan-Willem},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021},
  url = {https://proceedings.neurips.cc/paper/2021/hash/ab49b208848abe14418090d95df0d590-Abstract.html}
}

@article{girin2020dynamical,
  title={Dynamical variational autoencoders: A comprehensive review},
  author={Girin, Laurent and Leglaive, Simon and Bie, Xiaoyu and Diard, Julien and Hueber, Thomas and Alameda-Pineda, Xavier},
  journal={arXiv preprint arXiv:2008.12595},
  year={2020},
  url = {https://arxiv.org/abs/2008.12595}
}

@inproceedings{frankle2018lottery,
  title={The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks},
  author={Frankle, Jonathan and Carbin, Michael},
  booktitle={International Conference on Learning Representations},
  year={2018},
  url = {https://openreview.net/forum?id=rJl-b3RcF7}
}

@article{malach2020implications,
  title={The implications of local correlation on learning some deep functions},
  author={Malach, Eran and Shalev-Shwartz, Shai},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={1322--1332},
  year={2020},
  url = {https://proceedings.neurips.cc/paper/2020/hash/0e4ceef65add6cf21c0f3f9da53b71c0-Abstract.html}
}

@article{bronstein2021geometric,
  title={Geometric deep learning: Grids, groups, graphs, geodesics, and gauges},
  author={Bronstein, Michael M and Bruna, Joan and Cohen, Taco and Veli{\v{c}}kovi{\'c}, Petar},
  journal={arXiv preprint arXiv:2104.13478},
  year={2021},
  url = {https://arxiv.org/abs/2104.13478}
}

@article{keller2021topographic,
  title={Topographic {VAE}s learn equivariant capsules},
  author={Keller, Thomas and Welling, Max},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021},
  url = {https://proceedings.neurips.cc/paper/2021/hash/f03704cb51f02f80b09bffba15751691-Abstract.html}
}

@article{decelle2021equilibrium,
  title={Equilibrium and non-equilibrium regimes in the learning of restricted Boltzmann machines},
  author={Decelle, Aur{\'e}lien and Furtlehner, Cyril and Seoane, Beatriz},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021},
  url = {https://proceedings.neurips.cc/paper/2021/hash/2aedcba61ca55ceb62d785c6b7f10a83-Abstract.html}
}

@article{richthofer2017pfax,
  title={PFAx: Predictable Feature Analysis to Perform Control},
  author={Richthofer, Stefan and Wiskott, Laurenz},
  journal={arXiv preprint arXiv:1712.00634},
  year={2017},
  url = {https://arxiv.org/abs/1712.00634}
}

@article{wiskott2002slow,
  title={Slow feature analysis: Unsupervised learning of invariances},
  author={Wiskott, Laurenz and Sejnowski, Terrence J},
  journal={Neural computation},
  volume={14},
  number={4},
  pages={715--770},
  year={2002},
  publisher={MIT Press},
  url = {https://direct.mit.edu/neco/article-abstract/14/4/715/6583/Slow-Feature-Analysis-Unsupervised-Learning-of?redirectedFrom=fulltext}
}

@article{seth_theories_2022,
	title = {Theories of consciousness},
	rights = {2022 Springer Nature Limited},
	issn = {1471-0048},
	url = {https://www.nature.com/articles/s41583-022-00587-4},
	doi = {10.1038/s41583-022-00587-4},
	abstract = {Recent years have seen a blossoming of theories about the biological and physical basis of consciousness. Good theories guide empirical research, allowing us to interpret data, develop new experimental techniques and expand our capacity to manipulate the phenomenon of interest. Indeed, it is only when couched in terms of a theory that empirical discoveries can ultimately deliver a satisfying understanding of a phenomenon. However, in the case of consciousness, it is unclear how current theories relate to each other, or whether they can be empirically distinguished. To clarify this complicated landscape, we review four prominent theoretical approaches to consciousness: higher-order theories, global workspace theories, re-entry and predictive processing theories and integrated information theory. We describe the key characteristics of each approach by identifying which aspects of consciousness they propose to explain, what their neurobiological commitments are and what empirical data are adduced in their support. We consider how some prominent empirical debates might distinguish among these theories, and we outline three ways in which theories need to be developed to deliver a mature regimen of theory-testing in the neuroscience of consciousness. There are good reasons to think that the iterative development, testing and comparison of theories of consciousness will lead to a deeper understanding of this most profound of mysteries.},
	pages = {1--14},
	journaltitle = {Nature Reviews Neuroscience},
	shortjournal = {Nat Rev Neurosci},
	author = {Seth, Anil K. and Bayne, Tim},
	urldate = {2022-05-11},
	date = {2022-05-03},
	langid = {english},
	note = {Publisher: Nature Publishing Group},
	keywords = {Consciousness, Neuroscience},
	file = {Full Text PDF:/Users/work/Zotero/storage/6SZH73I6/Seth and Bayne - 2022 - Theories of consciousness.pdf:application/pdf;Snapshot:/Users/work/Zotero/storage/8KKHLGLB/s41583-022-00587-4.html:text/html},
}

@report{khan_bayesian_2022,
	title = {The Bayesian Learning Rule},
	url = {http://arxiv.org/abs/2107.04562},
	abstract = {We show that many machine-learning algorithms are specific instances of a single algorithm called the Bayesian learning rule. The rule, derived from Bayesian principles, yields a wide-range of algorithms from fields such as optimization, deep learning, and graphical models. This includes classical algorithms such as ridge regression, Newton's method, and Kalman filter, as well as modern deep-learning algorithms such as stochastic-gradient descent, {RMSprop}, and Dropout. The key idea in deriving such algorithms is to approximate the posterior using candidate distributions estimated by using natural gradients. Different candidate distributions result in different algorithms and further approximations to natural gradients give rise to variants of those algorithms. Our work not only unifies, generalizes, and improves existing algorithms, but also helps us design new ones.},
	number = {{arXiv}:2107.04562},
	institution = {{arXiv}},
	author = {Khan, Mohammad Emtiyaz and Rue, HÃ¥vard},
	urldate = {2022-06-01},
	date = {2022-03-18},
	doi = {10.48550/arXiv.2107.04562},
	eprinttype = {arxiv},
	eprint = {2107.04562 [cs, stat]},
	note = {type: article},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/work/Zotero/storage/BHNNJV77/Khan and Rue - 2022 - The Bayesian Learning Rule.pdf:application/pdf;arXiv.org Snapshot:/Users/work/Zotero/storage/6X2KQ3Y2/2107.html:text/html},
}

@report{zhou_fortuitous_2022,
	title = {Fortuitous Forgetting in Connectionist Networks},
	url = {http://arxiv.org/abs/2202.00155},
	abstract = {Forgetting is often seen as an unwanted characteristic in both human and machine learning. However, we propose that forgetting can in fact be favorable to learning. We introduce "forget-and-relearn" as a powerful paradigm for shaping the learning trajectories of artificial neural networks. In this process, the forgetting step selectively removes undesirable information from the model, and the relearning step reinforces features that are consistently useful under different conditions. The forget-and-relearn framework unifies many existing iterative training algorithms in the image classification and language emergence literature, and allows us to understand the success of these algorithms in terms of the disproportionate forgetting of undesirable information. We leverage this understanding to improve upon existing algorithms by designing more targeted forgetting operations. Insights from our analysis provide a coherent view on the dynamics of iterative training in neural networks and offer a clear path towards performance improvements.},
	number = {{arXiv}:2202.00155},
	institution = {{arXiv}},
	author = {Zhou, Hattie and Vani, Ankit and Larochelle, Hugo and Courville, Aaron},
	urldate = {2022-06-01},
	date = {2022-01-31},
	doi = {10.48550/arXiv.2202.00155},
	eprinttype = {arxiv},
	eprint = {2202.00155 [cs]},
	note = {type: article},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:/Users/work/Zotero/storage/TZI5ZV68/Zhou et al. - 2022 - Fortuitous Forgetting in Connectionist Networks.pdf:application/pdf;arXiv.org Snapshot:/Users/work/Zotero/storage/ZAAW3QWY/2202.html:text/html},
}

@report{lambert_variational_2022,
	title = {Variational inference via Wasserstein gradient flows},
	url = {http://arxiv.org/abs/2205.15902},
	abstract = {Along with Markov chain Monte Carlo ({MCMC}) methods, variational inference ({VI}) has emerged as a central computational approach to large-scale Bayesian inference. Rather than sampling from the true posterior \${\textbackslash}pi\$, {VI} aims at producing a simple but effective approximation \${\textbackslash}hat {\textbackslash}pi\$ to \${\textbackslash}pi\$ for which summary statistics are easy to compute. However, unlike the well-studied {MCMC} methodology, {VI} is still poorly understood and dominated by heuristics. In this work, we propose principled methods for {VI}, in which \${\textbackslash}hat {\textbackslash}pi\$ is taken to be a Gaussian or a mixture of Gaussians, which rest upon the theory of gradient flows on the Bures-Wasserstein space of Gaussian measures. Akin to {MCMC}, it comes with strong theoretical guarantees when \${\textbackslash}pi\$ is log-concave.},
	number = {{arXiv}:2205.15902},
	institution = {{arXiv}},
	author = {Lambert, Marc and Chewi, Sinho and Bach, Francis and Bonnabel, SilvÃ¨re and Rigollet, Philippe},
	urldate = {2022-06-01},
	date = {2022-05-31},
	doi = {10.48550/arXiv.2205.15902},
	eprinttype = {arxiv},
	eprint = {2205.15902 [cs, math, stat]},
	note = {type: article},
	keywords = {Computer Science - Machine Learning, Mathematics - Statistics Theory, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/work/Zotero/storage/UNBSQZ72/Lambert et al. - 2022 - Variational inference via Wasserstein gradient flo.pdf:application/pdf;arXiv.org Snapshot:/Users/work/Zotero/storage/LYC8HITV/2205.html:text/html},
}

@report{mediano_greater_2021,
	title = {Greater than the parts: A review of the information decomposition approach to causal emergence},
	url = {http://arxiv.org/abs/2111.06518},
	shorttitle = {Greater than the parts},
	abstract = {Emergence is a profound subject that straddles many scientific disciplines, including the formation of galaxies and how consciousness arises from the collective activity of neurons. Despite the broad interest that exists on this concept, the study of emergence has suffered from a lack of formalisms that could be used to guide discussions and advance theories. Here we summarise, elaborate on, and extend a recent formal theory of causal emergence based on information decomposition, which is quantifiable and amenable to empirical testing. This theory relates emergence with information about a system's temporal evolution that cannot be obtained from the parts of the system separately. This article provides an accessible but rigorous introduction to the framework, discussing the merits of the approach in various scenarios of interest. We also discuss several interpretation issues and potential misunderstandings, while highlighting the distinctive benefits of this formalism.},
	number = {{arXiv}:2111.06518},
	institution = {{arXiv}},
	author = {Mediano, Pedro A. M. and Rosas, Fernando E. and Luppi, Andrea I. and Jensen, Henrik J. and Seth, Anil K. and Barrett, Adam B. and Carhart-Harris, Robin L. and Bor, Daniel},
	urldate = {2022-06-01},
	date = {2021-11-11},
	doi = {10.48550/arXiv.2111.06518},
	eprinttype = {arxiv},
	eprint = {2111.06518 [nlin, q-bio]},
	note = {type: article},
	keywords = {Nonlinear Sciences - Adaptation and Self-Organizing Systems, Quantitative Biology - Neurons and Cognition},
	file = {arXiv Fulltext PDF:/Users/work/Zotero/storage/ATBXUYSI/Mediano et al. - 2021 - Greater than the parts A review of the informatio.pdf:application/pdf;arXiv.org Snapshot:/Users/work/Zotero/storage/2VUAZBN7/2111.html:text/html},
}

@report{ramesh_hierarchical_2022,
	title = {Hierarchical Text-Conditional Image Generation with {CLIP} Latents},
	url = {http://arxiv.org/abs/2204.06125},
	abstract = {Contrastive models like {CLIP} have been shown to learn robust representations of images that capture both semantics and style. To leverage these representations for image generation, we propose a two-stage model: a prior that generates a {CLIP} image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. We show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity. Our decoders conditioned on image representations can also produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation. Moreover, the joint embedding space of {CLIP} enables language-guided image manipulations in a zero-shot fashion. We use diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior, finding that the latter are computationally more efficient and produce higher-quality samples.},
	number = {{arXiv}:2204.06125},
	institution = {{arXiv}},
	author = {Ramesh, Aditya and Dhariwal, Prafulla and Nichol, Alex and Chu, Casey and Chen, Mark},
	urldate = {2022-06-03},
	date = {2022-04-12},
	doi = {10.48550/arXiv.2204.06125},
	eprinttype = {arxiv},
	eprint = {2204.06125 [cs]},
	note = {type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/work/Zotero/storage/TJDUQNEP/Ramesh et al. - 2022 - Hierarchical Text-Conditional Image Generation wit.pdf:application/pdf;arXiv.org Snapshot:/Users/work/Zotero/storage/QJ2NZEWQ/2204.html:text/html},
}

@report{reed_generalist_2022,
	title = {A Generalist Agent},
	url = {http://arxiv.org/abs/2205.06175},
	abstract = {Inspired by progress in large-scale language modeling, we apply a similar approach towards building a single generalist agent beyond the realm of text outputs. The agent, which we refer to as Gato, works as a multi-modal, multi-task, multi-embodiment generalist policy. The same network with the same weights can play Atari, caption images, chat, stack blocks with a real robot arm and much more, deciding based on its context whether to output text, joint torques, button presses, or other tokens. In this report we describe the model and the data, and document the current capabilities of Gato.},
	number = {{arXiv}:2205.06175},
	institution = {{arXiv}},
	author = {Reed, Scott and Zolna, Konrad and Parisotto, Emilio and Colmenarejo, Sergio Gomez and Novikov, Alexander and Barth-Maron, Gabriel and Gimenez, Mai and Sulsky, Yury and Kay, Jackie and Springenberg, Jost Tobias and Eccles, Tom and Bruce, Jake and Razavi, Ali and Edwards, Ashley and Heess, Nicolas and Chen, Yutian and Hadsell, Raia and Vinyals, Oriol and Bordbar, Mahyar and de Freitas, Nando},
	urldate = {2022-06-03},
	date = {2022-05-19},
	doi = {10.48550/arXiv.2205.06175},
	eprinttype = {arxiv},
	eprint = {2205.06175 [cs]},
	note = {type: article},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Robotics},
	file = {arXiv Fulltext PDF:/Users/work/Zotero/storage/AVYZ5AL6/Reed et al. - 2022 - A Generalist Agent.pdf:application/pdf;arXiv.org Snapshot:/Users/work/Zotero/storage/NBA7GESL/2205.html:text/html},
}

@report{locatello_challenging_2019,
	title = {Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations},
	url = {http://arxiv.org/abs/1811.12359},
	abstract = {The key idea behind the unsupervised learning of disentangled representations is that real-world data is generated by a few explanatory factors of variation which can be recovered by unsupervised learning algorithms. In this paper, we provide a sober look at recent progress in the field and challenge some common assumptions. We first theoretically show that the unsupervised learning of disentangled representations is fundamentally impossible without inductive biases on both the models and the data. Then, we train more than 12000 models covering most prominent methods and evaluation metrics in a reproducible large-scale experimental study on seven different data sets. We observe that while the different methods successfully enforce properties ``encouraged'' by the corresponding losses, well-disentangled models seemingly cannot be identified without supervision. Furthermore, increased disentanglement does not seem to lead to a decreased sample complexity of learning for downstream tasks. Our results suggest that future work on disentanglement learning should be explicit about the role of inductive biases and (implicit) supervision, investigate concrete benefits of enforcing disentanglement of the learned representations, and consider a reproducible experimental setup covering several data sets.},
	number = {{arXiv}:1811.12359},
	institution = {{arXiv}},
	author = {Locatello, Francesco and Bauer, Stefan and Lucic, Mario and RÃ¤tsch, Gunnar and Gelly, Sylvain and SchÃ¶lkopf, Bernhard and Bachem, Olivier},
	urldate = {2022-06-03},
	date = {2019-06-18},
	doi = {10.48550/arXiv.1811.12359},
	eprinttype = {arxiv},
	eprint = {1811.12359 [cs, stat]},
	note = {type: article},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/work/Zotero/storage/X5L24W6B/Locatello et al. - 2019 - Challenging Common Assumptions in the Unsupervised.pdf:application/pdf;arXiv.org Snapshot:/Users/work/Zotero/storage/N5ZRTQHN/1811.html:text/html},
}